{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU cuda\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2020)\n",
    "torch.manual_seed(2020)\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(2020)\n",
    "    print(\"Use GPU cuda\")\n",
    "else:\n",
    "    print(\"Use CPU\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>negative_prob</th>\n",
       "      <th>positive_prob</th>\n",
       "      <th>confidence</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.990446</td>\n",
       "      <td>0.009554</td>\n",
       "      <td>0.978769</td>\n",
       "      <td>这张三丰一点清新脱俗的世外高人的感觉都没有</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.600003</td>\n",
       "      <td>0.399997</td>\n",
       "      <td>0.111118</td>\n",
       "      <td>张五侠的铁画银钩被导演吃了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.996953</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>0.993228</td>\n",
       "      <td>我都没生，芷若都这么老了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.994541</td>\n",
       "      <td>0.005459</td>\n",
       "      <td>0.987868</td>\n",
       "      <td>老爸是最苦的啥都知道但啥都不能说可怜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.998267</td>\n",
       "      <td>0.996148</td>\n",
       "      <td>特别喜欢建国不知道为啥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21527</th>\n",
       "      <td>0</td>\n",
       "      <td>0.994942</td>\n",
       "      <td>0.005058</td>\n",
       "      <td>0.988759</td>\n",
       "      <td>那么多年，有没避孕措施，就生一个，奇怪...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21528</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.999897</td>\n",
       "      <td>0.999771</td>\n",
       "      <td>有史以来最帅俞岱岩，怎么下的去手，心疼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21529</th>\n",
       "      <td>0</td>\n",
       "      <td>0.995962</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>0.991027</td>\n",
       "      <td>就看不惯你俩！现在才多久就五哥素素的</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21530</th>\n",
       "      <td>0</td>\n",
       "      <td>0.923040</td>\n",
       "      <td>0.076960</td>\n",
       "      <td>0.828978</td>\n",
       "      <td>周芷若:丁师姐比芷若大不了几岁。这都还没芷若呢</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21531</th>\n",
       "      <td>1</td>\n",
       "      <td>0.049820</td>\n",
       "      <td>0.950180</td>\n",
       "      <td>0.889288</td>\n",
       "      <td>经典已成，只是来看赵敏周芷若得</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21532 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  negative_prob  positive_prob  confidence  \\\n",
       "0          0       0.990446       0.009554    0.978769   \n",
       "1          0       0.600003       0.399997    0.111118   \n",
       "2          0       0.996953       0.003047    0.993228   \n",
       "3          0       0.994541       0.005459    0.987868   \n",
       "4          1       0.001733       0.998267    0.996148   \n",
       "...      ...            ...            ...         ...   \n",
       "21527      0       0.994942       0.005058    0.988759   \n",
       "21528      1       0.000103       0.999897    0.999771   \n",
       "21529      0       0.995962       0.004038    0.991027   \n",
       "21530      0       0.923040       0.076960    0.828978   \n",
       "21531      1       0.049820       0.950180    0.889288   \n",
       "\n",
       "                      comments  \n",
       "0        这张三丰一点清新脱俗的世外高人的感觉都没有  \n",
       "1                张五侠的铁画银钩被导演吃了  \n",
       "2                 我都没生，芷若都这么老了  \n",
       "3           老爸是最苦的啥都知道但啥都不能说可怜  \n",
       "4                  特别喜欢建国不知道为啥  \n",
       "...                        ...  \n",
       "21527   那么多年，有没避孕措施，就生一个，奇怪...  \n",
       "21528      有史以来最帅俞岱岩，怎么下的去手，心疼  \n",
       "21529       就看不惯你俩！现在才多久就五哥素素的  \n",
       "21530  周芷若:丁师姐比芷若大不了几岁。这都还没芷若呢  \n",
       "21531          经典已成，只是来看赵敏周芷若得  \n",
       "\n",
       "[21532 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data=pd.read_csv('./online.csv',encoding='utf-8')\n",
    "# data=pd.read_csv('./chnsenticorp/new.csv',encoding = 'utf-8')\n",
    "data=pd.read_csv('./tencent.csv',encoding='utf-8')\n",
    "# data = data[2000:]\n",
    "# data=pd.read_csv('./data/ChnSentiCorp_htl_all.csv',encoding='utf-8')\n",
    "from transformers import BertTokenizer,BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./chinese-bert-wwm/\")\n",
    "data\n",
    "# result_comments=list(data['comments'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剔除标点符号,\\xa0 空格\n",
    "def pretreatment(comments):\n",
    "    result_comments=[]\n",
    "    punctuation='。，？！：%&~（）、；“”&|,.?!:%&~();\"\"'\n",
    "    for comment in comments:\n",
    "        comment= ''.join([c for c in comment if c not in punctuation])\n",
    "        comment= ''.join(comment.split())   #\\xa0\n",
    "        result_comments.append(comment)\n",
    "    \n",
    "    return result_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 6821, 6228, 7574, 3300, 4157, 691, 6205, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result_comments=pretreatment(list(data['comments'].values))\n",
    "result_comments=list(data['comments'].values)\n",
    "result = tokenizer.encode_plus('这视频有点东西')\n",
    "result\n",
    "# token = tokenizer.convert_ids_to_tokens(result)\n",
    "# token\n",
    "# result_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 6821, 2476,  ...,    0,    0,    0],\n",
       "        [ 101, 2476,  758,  ...,    0,    0,    0],\n",
       "        [ 101, 2769, 6963,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2218, 4692,  ...,    0,    0,    0],\n",
       "        [ 101, 1453, 5711,  ...,    0,    0,    0],\n",
       "        [ 101, 5307, 1073,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_comments_id=tokenizer(result_comments,padding=True,truncation=True,max_length=100,return_tensors='pt')\n",
    "result_comments_id\n",
    "# for i in range(len(result_comments)):\n",
    "#     result_comments_id[i] = tokenizer(result_comments[i],padding=True,truncation=True,max_length=200,return_tensors='pt')\n",
    "# result_comments_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21532, 62])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_comments_id['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=result_comments_id['input_ids']\n",
    "y=torch.from_numpy(data['label'].values).float()\n",
    "predict_data = pd.read_csv('./prediction.csv',encoding='utf-8')\n",
    "predict_comments=list(predict_data['comments'].values)\n",
    "predict_comments_id=tokenizer(predict_comments,padding=True,truncation=True,max_length=100,return_tensors='pt')\n",
    "X_pred = predict_comments_id['input_ids']\n",
    "y_pred = torch.from_numpy(predict_data['label'].values).float()\n",
    "X_train,X_test, y_train, y_test =train_test_split(X,y,test_size=0.3,shuffle=True,stratify=y,random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15072\n",
      "6460\n",
      "torch.Size([15072, 62])\n",
      "torch.Size([15072])\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3230, 3230)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid,X_test,y_valid,y_test=train_test_split(X_test,y_test,test_size=0.5,shuffle=True,stratify=y_test,random_state=2020)\n",
    "len(X_valid),len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "valid_data = TensorDataset(X_valid, y_valid)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "pred_data = TensorDataset(X_pred,y_pred)\n",
    "# print(X_test.shape)\n",
    "# dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "pred_loader = DataLoader(pred_data, shuffle=False, batch_size=10,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "if(USE_CUDA):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bert_lstm(nn.Module):\n",
    "    def __init__(self, hidden_dim,output_size,n_layers,bidirectional=True, drop_prob=0.5):\n",
    "        super(bert_lstm, self).__init__()\n",
    " \n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        #Bert ----------------重点，bert模型需要嵌入到自定义模型里面\n",
    "        self.bert=BertModel.from_pretrained(\"./chinese-bert-wwm/\")\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(768, hidden_dim, n_layers, batch_first=True,bidirectional=bidirectional)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim*2, output_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_size)\n",
    "          \n",
    "        #self.sig = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        #生成bert字向量\n",
    "        x=self.bert(x)[0]     #bert 字向量\n",
    "        \n",
    "        # lstm_out\n",
    "        #x = x.float()\n",
    "        lstm_out, (hidden_last,cn_last) = self.lstm(x, hidden)\n",
    "        #print(lstm_out.shape)   #[32,100,768]\n",
    "        #print(hidden_last.shape)   #[4, 32, 384]\n",
    "        #print(cn_last.shape)    #[4, 32, 384]\n",
    "        \n",
    "        #修改 双向的需要单独处理\n",
    "        if self.bidirectional:\n",
    "            #正向最后一层，最后一个时刻\n",
    "            hidden_last_L=hidden_last[-2]\n",
    "            #print(hidden_last_L.shape)  #[32, 384]\n",
    "            #反向最后一层，最后一个时刻\n",
    "            hidden_last_R=hidden_last[-1]\n",
    "            #print(hidden_last_R.shape)   #[32, 384]\n",
    "            #进行拼接\n",
    "            hidden_last_out=torch.cat([hidden_last_L,hidden_last_R],dim=-1)\n",
    "            #print(hidden_last_out.shape,'hidden_last_out')   #[32, 768]\n",
    "        else:\n",
    "            hidden_last_out=hidden_last[-1]   #[32, 384]\n",
    "            \n",
    "            \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(hidden_last_out)\n",
    "        #print(out.shape)    #[32,768]\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        number = 1\n",
    "        if self.bidirectional:\n",
    "            number = 2\n",
    "        \n",
    "        if (USE_CUDA):\n",
    "            hidden = (weight.new(self.n_layers*number, batch_size, self.hidden_dim).zero_().float().cuda(),\n",
    "                      weight.new(self.n_layers*number, batch_size, self.hidden_dim).zero_().float().cuda()\n",
    "                     )\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers*number, batch_size, self.hidden_dim).zero_().float(),\n",
    "                      weight.new(self.n_layers*number, batch_size, self.hidden_dim).zero_().float()\n",
    "                     )\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese-bert-wwm/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "output_size = 2\n",
    "hidden_dim = 384   #768/2\n",
    "n_layers = 6\n",
    "bidirectional = True  #这里为True，为双向LSTM\n",
    "\n",
    "net = bert_lstm(hidden_dim, output_size,n_layers, bidirectional)\n",
    "\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_82897/1400803172.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# move model to GPU, if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUSE_CUDA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# loss and optimization functions\n",
    "lr=2e-5\n",
    "epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "# training params\n",
    "\n",
    "# batch_size=50\n",
    "print_every = 10\n",
    "clip=5 # gradient clipping\n",
    " \n",
    "# move model to GPU, if available\n",
    "if(USE_CUDA):\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15072\n",
      "3230\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader.dataset))\n",
    "print(len(valid_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028\n",
      "1737\n",
      "113\n",
      "322\n",
      "Test loss: 0.326\n",
      "Epoch: 1/10... Loss: 0.481312... train accracy: 0.7767... Val Loss: 0.350450... valid accracy: 0.8477.\n",
      "\n",
      "Test accuracy: 0.8560 Sk test accuracy: 0.8641 F1: 0.8254 Precision: 0.9010 Recall: 0.7615\n",
      "1186\n",
      "1697\n",
      "153\n",
      "164\n",
      "Test loss: 0.259\n",
      "Epoch: 2/10... Loss: 0.282328... train accracy: 0.8875... Val Loss: 0.277742... valid accracy: 0.8728.\n",
      "\n",
      "Test accuracy: 0.8926 Sk test accuracy: 0.9009 F1: 0.8821 Precision: 0.8857 Recall: 0.8785\n",
      "1155\n",
      "1724\n",
      "125\n",
      "196\n",
      "Test loss: 0.278\n",
      "Epoch: 3/10... Loss: 0.207530... train accracy: 0.9275... Val Loss: 0.306935... valid accracy: 0.8765.\n",
      "\n",
      "Test accuracy: 0.8913 Sk test accuracy: 0.8997 F1: 0.8780 Precision: 0.9023 Recall: 0.8549\n",
      "1132\n",
      "1740\n",
      "112\n",
      "216\n",
      "Test loss: 0.342\n",
      "Epoch: 4/10... Loss: 0.149167... train accracy: 0.9532... Val Loss: 0.375534... valid accracy: 0.8786.\n",
      "\n",
      "Test accuracy: 0.8892 Sk test accuracy: 0.8975 F1: 0.8735 Precision: 0.9100 Recall: 0.8398\n",
      "1168\n",
      "1736\n",
      "112\n",
      "184\n",
      "Test loss: 0.338\n",
      "Epoch: 5/10... Loss: 0.121610... train accracy: 0.9654... Val Loss: 0.392291... valid accracy: 0.8820.\n",
      "\n",
      "Test accuracy: 0.8991 Sk test accuracy: 0.9075 F1: 0.8875 Precision: 0.9125 Recall: 0.8639\n",
      "1190\n",
      "1702\n",
      "153\n",
      "155\n",
      "Test loss: 0.308\n",
      "Epoch: 6/10... Loss: 0.093753... train accracy: 0.9745... Val Loss: 0.353553... valid accracy: 0.8839.\n",
      "\n",
      "Test accuracy: 0.8954 Sk test accuracy: 0.9038 F1: 0.8854 Precision: 0.8861 Recall: 0.8848\n",
      "1162\n",
      "1731\n",
      "122\n",
      "185\n",
      "Test loss: 0.422\n",
      "Epoch: 7/10... Loss: 0.066313... train accracy: 0.9845... Val Loss: 0.453783... valid accracy: 0.8873.\n",
      "\n",
      "Test accuracy: 0.8957 Sk test accuracy: 0.9041 F1: 0.8833 Precision: 0.9050 Recall: 0.8627\n",
      "1177\n",
      "1729\n",
      "121\n",
      "173\n",
      "Test loss: 0.425\n",
      "Epoch: 8/10... Loss: 0.050532... train accracy: 0.9890... Val Loss: 0.479273... valid accracy: 0.8882.\n",
      "\n",
      "Test accuracy: 0.8997 Sk test accuracy: 0.9081 F1: 0.8890 Precision: 0.9068 Recall: 0.8719\n",
      "1181\n",
      "1714\n",
      "135\n",
      "170\n",
      "Test loss: 0.466\n",
      "Epoch: 9/10... Loss: 0.041624... train accracy: 0.9915... Val Loss: 0.497772... valid accracy: 0.8901.\n",
      "\n",
      "Test accuracy: 0.8963 Sk test accuracy: 0.9047 F1: 0.8856 Precision: 0.8974 Recall: 0.8742\n",
      "1172\n",
      "1732\n",
      "118\n",
      "178\n",
      "Test loss: 0.471\n",
      "Epoch: 10/10... Loss: 0.035003... train accracy: 0.9931... Val Loss: 0.513644... valid accracy: 0.8901.\n",
      "\n",
      "Test accuracy: 0.8991 Sk test accuracy: 0.9075 F1: 0.8879 Precision: 0.9085 Recall: 0.8681\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    net.train()\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    counter = 0\n",
    "    train_acc = 0\n",
    "    valid_acc = 0\n",
    "    num_correct = 0\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        \n",
    "        if(USE_CUDA):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            # labels = labels.view(64, 1)\n",
    "            # print(labels.shape)\n",
    "        h = tuple([each.data for each in h])\n",
    "        net.zero_grad()\n",
    "        \n",
    "        output= net(inputs, h)\n",
    "        output.cuda()\n",
    "        output = output.squeeze()\n",
    "        # print(output.shape)\n",
    "        # print(labels)\n",
    "        # print(output)\n",
    "        loss = criterion(output, labels.long())\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        output=torch.nn.Softmax(dim=1)(output)\n",
    "        pred=torch.max(output, 1)[1]\n",
    "\n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not USE_CUDA else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        num_correct += np.sum(correct)\n",
    "        # print(num_correct)\n",
    "    train_acc = num_correct/len(train_loader.dataset)\n",
    "        # loss stats\n",
    "    num_correct = 0\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_h = net.init_hidden(batch_size)\n",
    "        # val_losses = []\n",
    "        for inputs, labels in valid_loader:\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "            if(USE_CUDA):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            output = net(inputs, val_h)\n",
    "            val_loss = criterion(output.squeeze(), labels.long())\n",
    "            valid_loss.append(val_loss.item())\n",
    "            output=torch.nn.Softmax(dim=1)(output)\n",
    "            pred=torch.max(output, 1)[1]\n",
    "\n",
    "                # compare predictions to true label\n",
    "            correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "            correct = np.squeeze(correct_tensor.numpy()) if not USE_CUDA else np.squeeze(correct_tensor.cpu().numpy())\n",
    "            num_correct += np.sum(correct)\n",
    "            # print(num_correct)\n",
    "        valid_acc = num_correct/len(valid_loader.dataset)\n",
    "    \n",
    "    #test\n",
    "    test_losses = [] # track loss\n",
    "    num_correct = 0\n",
    "    TN = 0\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    FP = 0 \n",
    "    # init hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    net.eval()\n",
    "    # iterate over test data\n",
    "    for inputs, labels in test_loader:\n",
    "        h = tuple([each.data for each in h])\n",
    "        if(USE_CUDA):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        output = net(inputs, h)\n",
    "        test_loss = criterion(output.squeeze(), labels.long())\n",
    "        test_losses.append(test_loss.item())\n",
    "        output=torch.nn.Softmax(dim=1)(output)\n",
    "        pred=torch.max(output, 1)[1]\n",
    "        pred_numpy = pred.detach().cpu().numpy()\n",
    "        labels_numpy = labels.detach().cpu().numpy()\n",
    "        # total_f1 += flat_f1(pred_numpy, labels_numpy)\n",
    "        # total_recall += flat_recall(pred_numpy, labels_numpy)\n",
    "        # total_precision += flat_precision(pred_numpy, labels_numpy)\n",
    "        # total_acc += flat_accuracy(pred_numpy, labels_numpy)\n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not USE_CUDA else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        num_correct += np.sum(correct)\n",
    "        TP += ((pred_numpy == 1) & (labels_numpy == 1)).sum()\n",
    "        TN += ((pred_numpy == 0) & (labels_numpy == 0)).sum()\n",
    "        FN += ((pred_numpy == 0) & (labels_numpy == 1)).sum()\n",
    "        FP += ((pred_numpy == 1) & (labels_numpy == 0)).sum()\n",
    "\n",
    "    print(TP)\n",
    "    print(TN)\n",
    "    print(FP)\n",
    "    print(FN)\n",
    "    p = TP / (TP + FP)\n",
    "    r = TP / (TP + FN)\n",
    "    F1 = 2 * r * p / (r + p)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "    \n",
    "    # accuracy over all test data\n",
    "    test_acc = num_correct/len(test_loader.dataset)\n",
    "    \n",
    "    # net.train()\n",
    "    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "          \"Loss: {:.6f}...\".format(np.mean(train_loss)),\n",
    "          \"train accracy: {:.4f}...\".format(train_acc),\n",
    "           \"Val Loss: {:.6f}...\".format(np.mean(valid_loss)),\n",
    "           \"valid accracy: {:.4f}.\".format(valid_acc))\n",
    "    print()\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_acc),\n",
    "    \"Sk test accuracy: {:.4f}\".format(acc),\n",
    "    \"F1: {:.4f}\".format(F1),\n",
    "    \"Precision: {:.4f}\".format(p),\n",
    "    \"Recall: {:.4f}\".format(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    \n",
    "    \"\"\"A function for calculating accuracy scores\"\"\"\n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return accuracy_score(labels_flat, pred_flat)\n",
    "def flat_f1(preds, labels):\n",
    "    \n",
    "    \"\"\"A function for calculating accuracy scores\"\"\"\n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, pred_flat)\n",
    "def flat_recall(preds, labels):\n",
    "    \n",
    "    \"\"\"A function for calculating accuracy scores\"\"\"\n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return recall_score(labels_flat, pred_flat)\n",
    "def flat_precision(preds, labels):\n",
    "    \n",
    "    \"\"\"A function for calculating accuracy scores\"\"\"\n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return precision_score(labels_flat, pred_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168\n",
      "1738\n",
      "118\n",
      "176\n",
      "Test loss: 0.468\n",
      "Test accuracy: 0.8997\n",
      "Sk test accuracy: 0.9081\n",
      "F1: 0.8882\n",
      "Precision: 0.9082\n",
      "Recall: 0.8690\n"
     ]
    }
   ],
   "source": [
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "TN = 0\n",
    "TP = 0\n",
    "FN = 0\n",
    "FP = 0 \n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    " \n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    if(USE_CUDA):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    output = net(inputs, h)\n",
    "    \n",
    "    test_loss = criterion(output.squeeze(), labels.long())\n",
    "    test_losses.append(test_loss.item())\n",
    "    output=torch.nn.Softmax(dim=1)(output)\n",
    "    # print(output.shape)\n",
    "    pred=torch.max(output, 1)[1]\n",
    "    pred_numpy = pred.detach().cpu().numpy()\n",
    "    labels_numpy = labels.detach().cpu().numpy()\n",
    "    # total_f1 += flat_f1(pred_numpy, labels_numpy)\n",
    "    # total_recall += flat_recall(pred_numpy, labels_numpy)\n",
    "    # total_precision += flat_precision(pred_numpy, labels_numpy)\n",
    "    # total_acc += flat_accuracy(pred_numpy, labels_numpy)\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not USE_CUDA else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    TP += ((pred_numpy == 1) & (labels_numpy == 1)).sum()\n",
    "    TN += ((pred_numpy == 0) & (labels_numpy == 0)).sum()\n",
    "    FN += ((pred_numpy == 0) & (labels_numpy == 1)).sum()\n",
    "    FP += ((pred_numpy == 1) & (labels_numpy == 0)).sum()\n",
    "\n",
    "print(TP)\n",
    "print(TN)\n",
    "print(FP)\n",
    "print(FN)\n",
    "p = TP / (TP + FP)\n",
    "r = TP / (TP + FN)\n",
    "F1 = 2 * r * p / (r + p)\n",
    "acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    " \n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.4f}\".format(test_acc))\n",
    "print(\"Sk test accuracy: {:.4f}\".format(acc))\n",
    "print(\"F1: {:.4f}\".format(F1))\n",
    "print(\"Precision: {:.4f}\".format(p))\n",
    "print(\"Recall: {:.4f}\".format(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'bert_lstm_tencent.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9395,  3.0506],\n",
      "        [ 2.7989, -2.7241],\n",
      "        [-2.9547,  3.0668],\n",
      "        [-2.9531,  3.0652],\n",
      "        [ 2.9301, -2.8449],\n",
      "        [-1.3944,  1.4334],\n",
      "        [ 3.2558, -3.1441],\n",
      "        [ 2.5662, -2.5126],\n",
      "        [ 3.2521, -3.1407],\n",
      "        [ 3.1321, -3.0314]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[0.0025, 0.9975],\n",
      "        [0.9960, 0.0040],\n",
      "        [0.0024, 0.9976],\n",
      "        [0.0024, 0.9976],\n",
      "        [0.9969, 0.0031],\n",
      "        [0.0558, 0.9442],\n",
      "        [0.9983, 0.0017],\n",
      "        [0.9938, 0.0062],\n",
      "        [0.9983, 0.0017],\n",
      "        [0.9979, 0.0021]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "[1 0 1 1 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# test_losses = [] # track loss\n",
    "# num_correct = 0\n",
    "# TN = 0\n",
    "# TP = 0\n",
    "# FN = 0\n",
    "# FP = 0 \n",
    "# # init hidden state\n",
    "net = torch.load('bert_lstm_tencent.pt')\n",
    "h = net.init_hidden(10)\n",
    " \n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in pred_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    if(USE_CUDA):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    output = net(inputs, h)\n",
    "    print(output)\n",
    "    # test_loss = criterion(output.squeeze(), labels.long())\n",
    "    # test_losses.append(test_loss.item())\n",
    "    output=torch.nn.Softmax(dim=1)(output)\n",
    "    # _,output = torch.max(output, 1)\n",
    "    # print(output)\n",
    "    pred=torch.max(output, 1)[1]\n",
    "    pred_numpy = pred.detach().cpu().numpy()\n",
    "    labels_numpy = labels.detach().cpu().numpy()\n",
    "    \n",
    "print(output)\n",
    "print(pred_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71bf23443dfe37405e827fd36ad8223143937c973cc476353c48d650d3e6b452"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('bert': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
