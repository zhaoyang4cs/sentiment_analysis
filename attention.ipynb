{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "SEED = 123\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3      #学习率\n",
    "EMBEDDING_DIM = 100       #词向量维度\n",
    "\n",
    "#为CPU设置随机种子\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "TEXT = data.Field(tokenize=lambda x: x.split(), lower=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "#get_dataset返回Dataset所需的examples和fields\n",
    "def get_dataset(corpur_path, text_field, label_field):\n",
    "    fields = [('text', text_field), ('label', label_field)]         #torchtext文件配对关系\n",
    "    examples = []\n",
    "\n",
    "    with open(corpur_path) as f:\n",
    "        li = []\n",
    "        while True:\n",
    "            content = f.readline().replace('\\n', '')\n",
    "            if not content:              #为空行，表示取完一次数据（一次的数据保存在li中）\n",
    "                if not li:               #如果列表也为空，则表示数据读完，结束循环\n",
    "                    break\n",
    "                label = li[0][10]\n",
    "                text = li[1][6:-7]\n",
    "                examples.append(data.Example.fromlist([text, label], fields))\n",
    "                li = []\n",
    "            else:\n",
    "                li.append(content)       #[\"<Polarity>标签</Polarity>\", \"<text>句子内容</text>\"]\n",
    "\n",
    "    return examples, fields\n",
    "\n",
    "#得到构建Dataset所需的examples和fields\n",
    "train_examples, train_fields = get_dataset(\"corpurs/trains.txt\", TEXT, LABEL)\n",
    "dev_examples, dev_fields = get_dataset(\"corpurs/dev.txt\", TEXT, LABEL)\n",
    "test_examples, test_fields = get_dataset(\"corpurs/tests.txt\", TEXT, LABEL)\n",
    "\n",
    "\n",
    "#构建Dataset数据集\n",
    "train_data = data.Dataset(train_examples, train_fields)\n",
    "dev_data = data.Dataset(dev_examples, dev_fields)\n",
    "test_data = data.Dataset(test_examples, test_fields)\n",
    "\n",
    "print('len of train data:', len(train_data))              #1000\n",
    "print('len of dev data:', len(dev_data))                  #200\n",
    "print('len of test data:', len(test_data))                #300\n",
    "\n",
    "print(train_data.examples[15].text)\n",
    "print(train_data.examples[15].label)\n",
    "\n",
    "\n",
    "#创建vocabulary\n",
    "TEXT.build_vocab(train_data, max_size=5000, vectors='glove.6B.100d')\n",
    "LABEL.build_vocab(train_data)\n",
    "print(len(TEXT.vocab))                     #3287\n",
    "print(TEXT.vocab.itos[:12])                #['<unk>', '<pad>', 'the', 'and', 'a', 'to', 'is', 'was', 'i', 'of', 'for', 'in']\n",
    "print(TEXT.vocab.stoi['like'])             #43\n",
    "print(LABEL.vocab.stoi)                    #defaultdict(None, {'0': 0, '1': 1})\n",
    "\n",
    "\n",
    "#创建iterators，每个itartion都会返回一个batch的examples\n",
    "train_iterator, dev_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, dev_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers):\n",
    "\n",
    "        super(BiLSTM_Attention, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    #x,query：[batch, seq_len, hidden_dim*2]\n",
    "    def attention_net(self, x, query, mask=None):      #软性注意力机制（key=value=x）\n",
    "\n",
    "        d_k = query.size(-1)                                              #d_k为query的维度\n",
    "        scores = torch.matmul(query, x.transpose(1, 2)) / math.sqrt(d_k)  #打分机制  scores:[batch, seq_len, seq_len]\n",
    "\n",
    "        p_attn = F.softmax(scores, dim = -1)                              #对最后一个维度归一化得分\n",
    "        context = torch.matmul(p_attn, x).sum(1)       #对权重化的x求和，[batch, seq_len, hidden_dim*2]->[batch, hidden_dim*2]\n",
    "        return context, p_attn\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.dropout(self.embedding(x))       #[seq_len, batch, embedding_dim]\n",
    "\n",
    "        # output: [seq_len, batch, hidden_dim*2]     hidden/cell: [n_layers*2, batch, hidden_dim]\n",
    "        output, (final_hidden_state, final_cell_state) = self.rnn(embedding)\n",
    "        output = output.permute(1, 0, 2)                  #[batch, seq_len, hidden_dim*2]\n",
    "\n",
    "        query = self.dropout(output)\n",
    "        attn_output, attention = self.attention_net(output, query)       #和LSTM的不同就在于这一句\n",
    "        logit = self.fc(attn_output)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = BiLSTM_Attention(len(TEXT.vocab), EMBEDDING_DIM, hidden_dim=64, n_layers=2)\n",
    "\n",
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "print('pretrained_embedding:', pretrained_embedding.shape)      #torch.Size([3287, 100])\n",
    "rnn.embedding.weight.data.copy_(pretrained_embedding)\n",
    "print('embedding layer inited.')\n",
    "\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=LEARNING_RATE)\n",
    "criteon = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算准确率\n",
    "def binary_acc(preds, y):\n",
    "    preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = torch.eq(preds, y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "\n",
    "#训练函数\n",
    "def train(rnn, iterator, optimizer, criteon):\n",
    "\n",
    "    avg_loss = []\n",
    "    avg_acc = []\n",
    "    rnn.train()        #表示进入训练模式\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "\n",
    "        pred = rnn(batch.text).squeeze()             #[batch, 1] -> [batch]\n",
    "\n",
    "        loss = criteon(pred, batch.label)\n",
    "        acc = binary_acc(pred, batch.label).item()   #计算每个batch的准确率\n",
    "\n",
    "        avg_loss.append(loss.item())\n",
    "        avg_acc.append(acc)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    avg_loss = np.array(avg_loss).mean()\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "#评估函数\n",
    "def evaluate(rnn, iterator, criteon):\n",
    "\n",
    "    avg_loss = []\n",
    "    avg_acc = []\n",
    "    rnn.eval()         #表示进入测试模式\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "\n",
    "            pred = rnn(batch.text).squeeze()        #[batch, 1] -> [batch]\n",
    "\n",
    "            loss = criteon(pred, batch.label)\n",
    "            acc = binary_acc(pred, batch.label).item()\n",
    "\n",
    "            avg_loss.append(loss.item())\n",
    "            avg_acc.append(acc)\n",
    "\n",
    "    avg_loss = np.array(avg_loss).mean()\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "#训练模型，并打印模型的表现\n",
    "best_valid_acc = float('-inf')\n",
    "\n",
    "for epoch in range(30):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(rnn, train_iterator, optimizer, criteon)\n",
    "    dev_loss, dev_acc = evaluate(rnn, dev_iterator, criteon)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "\n",
    "    if dev_acc > best_valid_acc:          #只要模型效果变好，就保存\n",
    "        best_valid_acc = dev_acc\n",
    "        torch.save(rnn.state_dict(), 'wordavg-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs:.2f}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {dev_loss:.3f} |  Val. Acc: {dev_acc*100:.2f}%')\n",
    "\n",
    "\n",
    "#用保存的模型参数预测数据\n",
    "rnn.load_state_dict(torch.load(\"wordavg-model.pt\"))\n",
    "test_loss, test_acc = evaluate(rnn, test_iterator, criteon)\n",
    "print(f'Test. Loss: {test_loss:.3f} |  Test. Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d0f776b7c0d8548d0bd1c347de978e312255c6f0532d8df378db6414ce71034"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('bert': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}