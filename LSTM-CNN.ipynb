{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data():\n",
    "#     data = []\n",
    "#     sample_num = 25000\n",
    "\n",
    "#     with open('./data/positive_process.data', 'r', encoding='utf-8') as f:\n",
    "#         sentences = f.readlines()\n",
    "#         for sentence in sentences[:sample_num]:\n",
    "#             words = [x for x in sentence.strip().split('\\t')]\n",
    "#             data.append([words, 0])\n",
    "\n",
    "#     with open('./data/negative_process.data', 'r', encoding='utf-8') as f:\n",
    "#         sentences = f.readlines()\n",
    "#         for sentence in sentences[:sample_num]:\n",
    "#             words = [x for x in sentence.strip().split('\\t')]\n",
    "#             data.append([words, 1])\n",
    "\n",
    "#     random.shuffle(data)\n",
    "#     return data\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "path = \"/home/zw/HDD/opinion/opinionExtraction/\"\n",
    "def load_data():\n",
    "    data = []\n",
    "    df = pd.read_csv(path + \"non_advice.csv\", encoding = \"utf-8\")\n",
    "    for sentence,label in zip(df[\"comments\"],df[\"label\"]):\n",
    "        words = sentence\n",
    "        sentiment = label\n",
    "        data.append([words,sentiment])\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "train_data, test_data = train_test_split(load_data(), test_size=0.2)\n",
    "# test_data, dev_data = train_test_split(test_data, test_size=0.5)\n",
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# words in vocab: 1595\n"
     ]
    }
   ],
   "source": [
    "def get_vocab(data):\n",
    "    tokenized_data = [words for words, _ in data]\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.Vocab(counter, min_freq=5)\n",
    "\n",
    "\n",
    "vocab = get_vocab(train_data)\n",
    "print('# words in vocab:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, vocab):\n",
    "    max_l = 100  # 将每条评论通过截断或者补0，使得长度变成500\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data = [words for words, _ in data]\n",
    "    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_set = Data.TensorDataset(*preprocess(train_data, vocab))\n",
    "test_set = Data.TensorDataset(*preprocess(test_data, vocab))\n",
    "# dev_set = Data.TensorDataset(*preprocess(dev_data, vocab))\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)\n",
    "# dev_iter = Data.DataLoader(dev_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X torch.Size([16, 100]) y torch.Size([16])\n#batches: 946\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "print('#batches:', len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        # bidirectional设为True即得到双向循环神经网络\n",
    "        self.encoder = nn.LSTM(input_size=embed_size,\n",
    "                               hidden_size=num_hiddens,\n",
    "                               num_layers=num_layers,\n",
    "                               bidirectional=True)\n",
    "        # 初始时间步和最终时间步的隐藏状态作为全连接层输入\n",
    "        self.decoder = nn.Linear(4 * num_hiddens, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs的形状是(批量大小, 词数)，因为LSTM需要将序列长度(seq_len)作为第一维，所以将输入转置后\n",
    "        # 再提取词特征，输出形状为(词数, 批量大小, 词向量维度)\n",
    "        embeddings = self.embedding(inputs.permute(1, 0))\n",
    "        # rnn.LSTM只传入输入embeddings，因此只返回最后一层的隐藏层在各时间步的隐藏状态。\n",
    "        # outputs形状是(词数, 批量大小, 2 * 隐藏单元个数)\n",
    "        outputs, _ = self.encoder(embeddings)  # output, (h, c)\n",
    "        # 连结初始时间步和最终时间步的隐藏状态作为全连接层输入。它的形状为\n",
    "        # (批量大小, 4 * 隐藏单元个数)。\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs\n",
    "\n",
    "# class BiLSTM_Attention(nn.Module):\n",
    "#     def __init__(self, vocab, embedding_dim, num_hiddens, num_layers):\n",
    "#         super(BiLSTM_Attention, self).__init__()\n",
    "#         self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
    "#         # bidirectional设为True即得到双向循环神经网络\n",
    "#         self.encoder = nn.LSTM(input_size=embedding_dim,\n",
    "#                                hidden_size=num_hiddens,\n",
    "#                                num_layers=num_layers,\n",
    "#                                batch_first=True,\n",
    "#                                bidirectional=True)\n",
    "#         self.w_omega = nn.Parameter(torch.Tensor(\n",
    "#             num_hiddens * 2, num_hiddens * 2))\n",
    "#         self.u_omega = nn.Parameter(torch.Tensor(num_hiddens * 2, 1))\n",
    "#         self.decoder = nn.Linear(4*num_hiddens, 2)\n",
    "\n",
    "#         nn.init.uniform_(self.w_omega, -0.1, 0.1)\n",
    "#         nn.init.uniform_(self.u_omega, -0.1, 0.1)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         # inputs的形状是(seq_len,batch_size)\n",
    "#         embeddings = self.embedding(inputs.permute(1, 0))\n",
    "#         # 提取词特征，输出形状为(seq_len,batch_size,embedding_dim)\n",
    "#         # rnn.LSTM只返回最后一层的隐藏层在各时间步的隐藏状态。\n",
    "#         outputs, _ = self.encoder(embeddings)  # output, (h, c)\n",
    "#         # outputs形状是(seq_len,batch_size, 2 * num_hiddens)\n",
    "#         x = outputs.permute(1, 0, 2)\n",
    "#         # x形状是(batch_size, seq_len, 2 * num_hiddens)\n",
    "       \n",
    "#         # Attention过程\n",
    "#         u = torch.tanh(torch.matmul(x, self.w_omega))\n",
    "#        # u形状是(batch_size, seq_len, 2 * num_hiddens)\n",
    "#         att = torch.matmul(u, self.u_omega)\n",
    "#        # att形状是(batch_size, seq_len, 1)\n",
    "#         att_score = F.softmax(att, dim=1)\n",
    "#        # att_score形状仍为(batch_size, seq_len, 1)\n",
    "#         scored_x = x * att_score\n",
    "#        # scored_x形状是(batch_size, seq_len, 2 * num_hiddens)\n",
    "#         # Attention过程结束\n",
    "       \n",
    "#         feat = torch.sum(scored_x, dim=1)\n",
    "#        # feat形状是(batch_size, 2 * num_hiddens)\n",
    "#         outs = self.decoder(feat)\n",
    "#        # out形状是(batch_size, 2)\n",
    "#         return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = '.vector_cache'\n",
    "if not os.path.exists(cache):\n",
    "    os.mkdir(cache)\n",
    "glove_vocab = Vocab.Vectors(name='./douyin_work/sgns.renmin.bigram-char', cache=cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self, vocab, embedding_dim, num_hiddens, num_layers):\n",
    "        super(BiLSTM_Attention, self).__init__()\n",
    "        # embedding之后的shape: torch.Size([200, 8, 300])\n",
    "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
    "        # self.embedding = self.embedding.from_pretrained(vocab.vectors, freeze=False)\n",
    "        # bidirectional设为True即得到双向循环神经网络\n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim,\n",
    "                               hidden_size=num_hiddens,\n",
    "                               num_layers=num_layers,\n",
    "                               batch_first=False,\n",
    "                               bidirectional=True)\n",
    "        # 初始时间步和最终时间步的隐藏状态作为全连接层输入\n",
    "        self.w_omega = nn.Parameter(torch.Tensor(\n",
    "            num_hiddens * 2, num_hiddens * 2))\n",
    "        self.u_omega = nn.Parameter(torch.Tensor(num_hiddens * 2, 1))\n",
    "        self.decoder = nn.Linear(2*num_hiddens, 2)\n",
    "\n",
    "        nn.init.uniform_(self.w_omega, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.u_omega, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs的形状是(seq_len,batch_size)\n",
    "        embeddings = self.embedding(inputs.permute(1, 0))\n",
    "        # print(embeddings.size())\n",
    "        # 提取词特征，输出形状为(seq_len,batch_size,embedding_dim)\n",
    "        # rnn.LSTM只返回最后一层的隐藏层在各时间步的隐藏状态。\n",
    "        outputs, _ = self.encoder(embeddings)  # output, (h, c)\n",
    "        # outputs形状是(seq_len,batch_size, 2 * num_hiddens)\n",
    "        x = outputs.permute(1, 0, 2)\n",
    "        # print(x.size())\n",
    "        # x形状是(batch_size, seq_len, 2 * num_hiddens)\n",
    "        \n",
    "        # Attention过程\n",
    "        u = torch.tanh(torch.matmul(x, self.w_omega))\n",
    "       # u形状是(batch_size, seq_len, 2 * num_hiddens)\n",
    "        att = torch.matmul(u, self.u_omega)\n",
    "       # att形状是(batch_size, seq_len, 1)\n",
    "        att_score = F.softmax(att, dim=1)\n",
    "       # att_score形状仍为(batch_size, seq_len, 1)\n",
    "        scored_x = x * att_score\n",
    "       # scored_x形状是(batch_size, seq_len, 2 * num_hiddens)\n",
    "        # Attention过程结束\n",
    "        \n",
    "        feat = torch.sum(scored_x, dim=1)\n",
    "       # feat形状是(batch_size, 2 * num_hiddens)\n",
    "        outs = self.decoder(feat)\n",
    "        # print(outs.size())\n",
    "       # out形状是(batch_size, 2)\n",
    "        return outs\n",
    "vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers = 300, 300, 2\n",
    "net = BiLSTM_Attention(vocab, embed_size, num_hiddens, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are 34 oov words.\n"
     ]
    }
   ],
   "source": [
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    \"\"\"从预训练好的vocab中提取出words对应的词向量\"\"\"\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0])  # 初始化为0\n",
    "    oov_count = 0  # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "    return embed\n",
    "\n",
    "\n",
    "net.embedding.weight.data.copy_(\n",
    "    load_pretrained_embedding(vocab.itos, glove_vocab))\n",
    "net.embedding.weight.requires_grad = False  # 直接加载预训练好的, 所以不需要更新它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 4/946 [00:00<00:31, 30.28it/s]training on  cuda\n",
      "100%|██████████| 946/946 [00:30<00:00, 31.52it/s]\n",
      "  0%|          | 4/946 [00:00<00:29, 31.69it/s]epoch 1, loss 0.3324, train acc 0.842, test acc 0.941, precision 0.936, R 0.958, F1 0.947, time 32.0 sec\n",
      "100%|██████████| 946/946 [00:32<00:00, 28.92it/s]\n",
      "  0%|          | 3/946 [00:00<00:32, 29.21it/s]epoch 2, loss 0.0751, train acc 0.945, test acc 0.950, precision 0.940, R 0.972, F1 0.956, time 34.7 sec\n",
      "100%|██████████| 946/946 [00:33<00:00, 28.40it/s]\n",
      "  0%|          | 3/946 [00:00<00:32, 29.01it/s]epoch 3, loss 0.0331, train acc 0.965, test acc 0.964, precision 0.970, R 0.964, F1 0.967, time 35.3 sec\n",
      "100%|██████████| 946/946 [00:33<00:00, 28.41it/s]\n",
      "  0%|          | 4/946 [00:00<00:29, 32.10it/s]epoch 4, loss 0.0180, train acc 0.974, test acc 0.964, precision 0.975, R 0.959, F1 0.967, time 35.2 sec\n",
      "100%|██████████| 946/946 [00:32<00:00, 29.03it/s]\n",
      "  0%|          | 4/946 [00:00<00:27, 34.38it/s]epoch 5, loss 0.0104, train acc 0.982, test acc 0.966, precision 0.977, R 0.962, F1 0.969, time 34.6 sec\n",
      "100%|██████████| 946/946 [00:32<00:00, 29.12it/s]\n",
      "  0%|          | 4/946 [00:00<00:30, 30.69it/s]epoch 6, loss 0.0098, train acc 0.979, test acc 0.965, precision 0.961, R 0.975, F1 0.968, time 34.4 sec\n",
      "100%|██████████| 946/946 [00:32<00:00, 28.81it/s]\n",
      "  0%|          | 4/946 [00:00<00:28, 33.47it/s]epoch 7, loss 0.0062, train acc 0.984, test acc 0.965, precision 0.971, R 0.966, F1 0.968, time 34.8 sec\n",
      "100%|██████████| 946/946 [00:32<00:00, 29.42it/s]\n",
      "  0%|          | 3/946 [00:00<00:34, 27.40it/s]epoch 8, loss 0.0036, train acc 0.990, test acc 0.965, precision 0.971, R 0.965, F1 0.968, time 34.0 sec\n",
      "100%|██████████| 946/946 [00:32<00:00, 29.27it/s]\n",
      "  0%|          | 4/946 [00:00<00:29, 31.89it/s]epoch 9, loss 0.0034, train acc 0.990, test acc 0.954, precision 0.988, R 0.928, F1 0.957, time 34.3 sec\n",
      "100%|██████████| 946/946 [00:31<00:00, 29.82it/s]\n",
      "  0%|          | 4/946 [00:00<00:29, 32.27it/s]epoch 10, loss 0.0034, train acc 0.988, test acc 0.963, precision 0.980, R 0.952, F1 0.966, time 33.7 sec\n",
      "100%|██████████| 946/946 [00:32<00:00, 28.67it/s]\n",
      "  0%|          | 4/946 [00:00<00:29, 31.75it/s]epoch 11, loss 0.0022, train acc 0.992, test acc 0.966, precision 0.963, R 0.977, F1 0.970, time 34.9 sec\n",
      "100%|██████████| 946/946 [00:31<00:00, 30.29it/s]\n",
      "  0%|          | 4/946 [00:00<00:29, 31.88it/s]epoch 12, loss 0.0019, train acc 0.993, test acc 0.965, precision 0.972, R 0.964, F1 0.968, time 33.2 sec\n",
      "100%|██████████| 946/946 [00:30<00:00, 30.55it/s]\n",
      "  0%|          | 4/946 [00:00<00:29, 32.40it/s]epoch 13, loss 0.0016, train acc 0.993, test acc 0.969, precision 0.972, R 0.971, F1 0.971, time 32.9 sec\n",
      "100%|██████████| 946/946 [00:30<00:00, 30.64it/s]\n",
      "  0%|          | 4/946 [00:00<00:28, 32.75it/s]epoch 14, loss 0.0017, train acc 0.992, test acc 0.966, precision 0.977, R 0.960, F1 0.968, time 32.8 sec\n",
      "100%|██████████| 946/946 [00:31<00:00, 30.07it/s]\n",
      "  0%|          | 4/946 [00:00<00:29, 31.94it/s]epoch 15, loss 0.0012, train acc 0.994, test acc 0.967, precision 0.967, R 0.972, F1 0.970, time 33.4 sec\n",
      "100%|██████████| 946/946 [00:31<00:00, 30.10it/s]\n",
      "  0%|          | 4/946 [00:00<00:29, 31.77it/s]epoch 16, loss 0.0010, train acc 0.995, test acc 0.965, precision 0.962, R 0.975, F1 0.968, time 33.3 sec\n",
      "100%|██████████| 946/946 [00:31<00:00, 30.06it/s]\n",
      "  0%|          | 4/946 [00:00<00:29, 32.03it/s]epoch 17, loss 0.0008, train acc 0.996, test acc 0.967, precision 0.969, R 0.972, F1 0.970, time 33.4 sec\n",
      "100%|██████████| 946/946 [00:29<00:00, 31.62it/s]\n",
      "  0%|          | 4/946 [00:00<00:29, 31.54it/s]epoch 18, loss 0.0008, train acc 0.995, test acc 0.964, precision 0.973, R 0.962, F1 0.967, time 31.9 sec\n",
      "100%|██████████| 946/946 [00:31<00:00, 29.77it/s]\n",
      "  0%|          | 4/946 [00:00<00:28, 32.70it/s]epoch 19, loss 0.0009, train acc 0.994, test acc 0.963, precision 0.963, R 0.969, F1 0.966, time 33.7 sec\n",
      "100%|██████████| 946/946 [00:31<00:00, 30.35it/s]\n",
      "epoch 20, loss 0.0007, train acc 0.996, test acc 0.965, precision 0.981, R 0.956, F1 0.968, time 33.1 sec\n"
     ]
    }
   ],
   "source": [
    "def evaluate(data_iter, net, device=None): # 测试函数\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # 如果没指定device就使用net的device\n",
    "        device = list(net.parameters())[0].device\n",
    "    TP, TN, FP, FN = 0.0, 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter: \n",
    "            net.eval()  # 评估模式, 这会关闭dropout\n",
    "            # net(X.to(device)).argmax(dim=1)\n",
    "            pred = net(X.to(device)).argmax(dim=1).cpu()\n",
    "            test = y.to(device).cpu()\n",
    "            net.train()  # 改回训练模式\n",
    "            TP += ((pred == 1) & (test == 1)).cpu().sum()\n",
    "            # TN    predict 和 label 同时为0\n",
    "            TN += ((pred == 0) & (test == 0)).cpu().sum()\n",
    "            # FN    predict 0 label 1\n",
    "            FN += ((pred == 0) & (test == 1)).cpu().sum()\n",
    "            # FP    predict 1 label 0\n",
    "            FP += ((pred == 1) & (test == 0)).cpu().sum()\n",
    "        p = TP / (TP + FP)\n",
    "        r = TP / (TP + FN)\n",
    "        F1 = 2 * r * p / (r + p)\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN)       \n",
    "    return acc, p, r, F1\n",
    "from sklearn.metrics import accuracy_score\n",
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # 如果没指定device就使用net的device\n",
    "        device = list(net.parameters())[0].device\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            net.eval()  # 评估模式, 这会关闭dropout\n",
    "            acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "            net.train()  # 改回训练模式\n",
    "            n += y.shape[0]\n",
    "\n",
    "    return acc_sum/n\n",
    "\n",
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs): # 训练函数\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in tqdm(train_iter):\n",
    "            # target = target.view(args.batch_size)\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            # print(y)\n",
    "            # print(y_hat)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc, test_precision, test_R, test_F1 = evaluate(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, precision %.3f, R %.3f, F1 %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, test_precision, test_R, test_F1, time.time() - start))\n",
    "        # print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "        #       % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n",
    "\n",
    "lr, num_epochs = 0.003, 20\n",
    "# 要过滤掉不计算梯度的embedding参数\n",
    "net.train()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)\n",
    "#计算准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  6%|▌         | 12/206 [00:00<00:01, 112.25it/s]There are 17 oov words.\n",
      "There are 17 oov words.\n",
      "training on  cuda\n",
      "100%|██████████| 206/206 [00:01<00:00, 112.78it/s]\n",
      "  6%|▌         | 12/206 [00:00<00:01, 117.53it/s]epoch 1, loss 0.4713, train acc 0.773, test acc 0.805, time 2.0 sec\n",
      "100%|██████████| 206/206 [00:01<00:00, 115.33it/s]\n",
      "  6%|▌         | 12/206 [00:00<00:01, 112.87it/s]epoch 2, loss 0.1815, train acc 0.834, test acc 0.842, time 1.9 sec\n",
      "100%|██████████| 206/206 [00:01<00:00, 111.52it/s]\n",
      "  6%|▌         | 12/206 [00:00<00:01, 114.03it/s]epoch 3, loss 0.0944, train acc 0.873, test acc 0.847, time 2.0 sec\n",
      "100%|██████████| 206/206 [00:01<00:00, 110.97it/s]\n",
      "  6%|▌         | 12/206 [00:00<00:01, 112.17it/s]epoch 4, loss 0.0567, train acc 0.901, test acc 0.838, time 2.0 sec\n",
      "100%|██████████| 206/206 [00:01<00:00, 111.08it/s]\n",
      "epoch 5, loss 0.0374, train acc 0.929, test acc 0.846, time 2.0 sec\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "import collections\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def load_data():\n",
    "    data = []\n",
    "    df = pd.read_csv(\"./douyin_f.csv\", encoding = \"utf-8\")\n",
    "    for sentence,label in zip(df[\"comments\"],df[\"label\"]):\n",
    "        words = sentence\n",
    "        sentiment = label\n",
    "        data.append([words,sentiment])\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "def get_vocab(data):\n",
    "    tokenized_data = [words for words, _ in data]\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.Vocab(counter, min_freq=5)\n",
    "\n",
    "\n",
    "def preprocess(data, vocab):\n",
    "    max_l = 100  # 将每条评论通过截断或者补0，使得长度变成500\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data = [words for words, _ in data]\n",
    "    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    \"\"\"从预训练好的vocab中提取出words对应的词向量\"\"\"\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0])  # 初始化为0\n",
    "    oov_count = 0  # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "    return embed\n",
    "\n",
    "\n",
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # 如果没指定device就使用net的device\n",
    "        device = list(net.parameters())[0].device\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            net.eval()  # 评估模式, 这会关闭dropout\n",
    "            acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "            net.train()  # 改回训练模式\n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "\n",
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    opt_test_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in tqdm(train_iter):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n",
    "\n",
    "train_data, test_data = train_test_split(load_data(), test_size=0.2)\n",
    "batch_size = 32\n",
    "vocab = get_vocab(train_data)\n",
    "train_set = Data.TensorDataset(*preprocess(train_data, vocab))\n",
    "test_set = Data.TensorDataset(*preprocess(test_data, vocab))\n",
    "# dev_set = Data.TensorDataset(*preprocess(dev_data, vocab))\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)\n",
    "\n",
    "\n",
    "class GlobalMaxPool1d(nn.Module): # 用一维池化层实现时序最大池化层\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channel, seq_len)\n",
    "        # return shape: (batch_size, channel, 1)\n",
    "        return F.max_pool1d(x, kernel_size=x.shape[2])\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, kernel_sizes, num_channels):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        # 不参与训练的嵌入层\n",
    "        self.constant_embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Linear(sum(num_channels), 2)\n",
    "        # 时序最大池化层没有权重，所以可以共用一个实例\n",
    "        self.pool = GlobalMaxPool1d()\n",
    "        self.convs = nn.ModuleList()  # 创建多个一维卷积层\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(in_channels=2 * embed_size,\n",
    "                                        out_channels=c,\n",
    "                                        kernel_size=k))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 将两个形状是(批量大小, 词数, 词向量维度)的嵌入层的输出按词向量连结\n",
    "        embeddings = torch.cat((\n",
    "            self.embedding(inputs),\n",
    "            self.constant_embedding(inputs)), dim=2)  # (batch, seq_len, 2*embed_size)\n",
    "        # 根据Conv1D要求的输入格式，将词向量维，即一维卷积层的通道维(即词向量那一维)，变换到前一维\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        # 对于每个一维卷积层，在时序最大池化后会得到一个形状为(批量大小, 通道大小, 1)的\n",
    "        # Tensor。使用flatten函数去掉最后一维，然后在通道维上连结\n",
    "        encoding = torch.cat([self.pool(F.relu(conv(embeddings))).squeeze(-1) for conv in self.convs], dim=1)\n",
    "        # 应用丢弃法后使用全连接层得到输出\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "embed_size, kernel_sizes, nums_channels = 300, [3, 4, 5], [300, 300, 300]\n",
    "net = TextCNN(vocab, embed_size, kernel_sizes, nums_channels)\n",
    "\n",
    "cache = '.vector_cache'\n",
    "if not os.path.exists(cache):\n",
    "    os.mkdir(cache)\n",
    "glove_vocab = Vocab.Vectors(name='./douyin_work/sgns.renmin.bigram-char', cache=cache)\n",
    "net.embedding.weight.data.copy_(\n",
    "    load_pretrained_embedding(vocab.itos, glove_vocab))\n",
    "net.constant_embedding.weight.data.copy_(\n",
    "    load_pretrained_embedding(vocab.itos, glove_vocab))\n",
    "net.constant_embedding.weight.requires_grad = False\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# path = \"/home/zw/HDD/opinion/opinionExtraction/\"\n",
    "# f = open(path + \"non_advice.csv\",\"r\")\n",
    "# positive = list()\n",
    "# negetive = list()\n",
    "# positive.append(\"label,negative_prob,positive_prob,confidence,comments\")\n",
    "# negetive.append(\"label,negative_prob,positive_prob,confidence,comments\")\n",
    "# for line in open(path + \"gov_final.csv\",\"r\"):\n",
    "#     line = f.readline()\n",
    "#     str = line.split(\",\")\n",
    "#     if str[0] == '1':\n",
    "#         positive.append(line)\n",
    "#     if str[0] == \"0\":\n",
    "#         negetive.append(line)\n",
    "# f.close()\n",
    "# open(path + 'p_result.csv', 'w').write('%s' % \"\".join(positive))\n",
    "# open(path + 'n_result.csv', 'w').write('%s' % \"\".join(negetive))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d0f776b7c0d8548d0bd1c347de978e312255c6f0532d8df378db6414ce71034"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('bert': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}