{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part2: bert feature-base\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as tfs\n",
    "import warnings\n",
    "from sklearn.utils import shuffle\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('./two_result.csv', encoding = \"utf-8\")\n",
    "train_df = pd.read_csv('./online.csv', encoding = \"utf-8\")\n",
    "train_df = train_df[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretreatment(comments):\n",
    "    result_comments=[]\n",
    "    punctuation='。，？！：%&~（）、；“”&|,.?!:%&~();\"\"'\n",
    "    for comment in comments:\n",
    "        comment= ''.join([c for c in comment if c not in punctuation])\n",
    "        comment= ''.join(comment.split())   #\\xa0\n",
    "        result_comments.append(comment)\n",
    "    \n",
    "    return result_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set shape: (20000, 3)\n"
     ]
    }
   ],
   "source": [
    "train_set = train_df\n",
    "\n",
    " \n",
    "print(\"Test set shape:\", train_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1    12091\n0     7909\nName: label, dtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese-bert-wwm/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./chinese-bert-wwm/\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"./chinese-bert-wwm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = train_set['comments'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True,max_length=512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set shape: (20000, 512)\n"
     ]
    }
   ],
   "source": [
    "train_max_len = 0\n",
    "for i in train_tokenized.values:\n",
    "    if len(i) > train_max_len:\n",
    "        train_max_len = len(i)\n",
    "\n",
    "train_padded = np.array([i + [0] * (train_max_len-len(i)) for i in train_tokenized.values])\n",
    "print(\"train set shape:\",train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 101  976 4266 3678  671 2137 6206 3300 1155 1864 6821 3416 4638 2552\n",
      " 2578 8024  679 3171 1765 2110  739 8024  679 3171 1765 6822 3635 8024\n",
      "  679 3171 1765 5314 5632 2346 6133 1041 3173 7831 6117 3890 8024 6375\n",
      " 5632 2346  924 2898  671 7578 2399 6768 4638 2552  511 2769 2682 8024\n",
      " 6821 3221  800 5543 2523 1962 4638 1469 2111 2094 3765 6858 4638  671\n",
      "  702 7028 6206 1728 5162  511 6438 1155 1864 4638 3152 4995 8024 2600\n",
      " 5543 6375 2769 4692 1168  671  702 2571  727 4638 2398 3211 6818  782\n",
      " 4638 4266  779 8024  800 1993 5303 4991 1762 1469 2111 2094 1398 3416\n",
      " 4638 7770 2428 8024 5314 2111 2094 1158 6863 4708  671  702 1041 4007\n",
      " 4263 1469 5632 4507 4638 4495 3833 4384 1862  511 2523 1599 3614 1155\n",
      " 1864 1762 2099 7027 6121 7313 3837 7463 1139 4638  976 4266 3678 4638\n",
      " 6929 4905 2207 4322 7954 8024 6375  782 2600 3221 2556  916  679 4881\n",
      " 8024 4266 3678 1469 2094 1957  722 7313 3300 3198  952  738 3221  671\n",
      " 4905 2773 3159 8024 3636 1213  751 3159 6814  754  856 5277  749 8024\n",
      " 3255 1213 6772 7030 2798 3291 3300 6637 1456  511 2792  809 8024  976\n",
      " 4266 3678 4638 2533 1217 2828 1226  749 8024 5439 2590 2682 5439 6225\n",
      " 2573 3800 2137  833  671 6571 3864 1765 8024 4495 1462  679 2622 8024\n",
      " 2110  739  679 3632  511 2157 2431 3136 5509 8024 4696 4638 3221  727\n",
      " 1762 1071  704  511  102    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_padded[0])\n",
    "train_attention_mask = np.where(train_padded != 0, 1, 0)\n",
    "print(train_attention_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集\n",
    "train_input_ids = torch.tensor(train_padded).long()\n",
    "train_attention_mask = torch.tensor(train_attention_mask).long()\n",
    "with torch.no_grad():\n",
    "    train_last_hidden_states = model(train_input_ids, attention_mask=train_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 512, 21128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_last_hidden_states[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       1\n",
      "2       0\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "1995    1\n",
      "1996    1\n",
      "1997    0\n",
      "1998    1\n",
      "1999    1\n",
      "Name: label, Length: 2000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print(train_last_hidden_states[0][:,0,:].numpy())\n",
    "print(train_set['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_last_hidden_states[0][:,0,:].numpy()\n",
    "train_labels = train_set['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.852"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 2 - bert fine-tuned\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import transformers as tfs\n",
    "import math\n",
    "\n",
    "class BertClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassificationModel, self).__init__()   \n",
    "        # from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "        # self.model = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\")\n",
    "        model_class, tokenizer_class, pretrained_weights = (tfs.BertModel, tfs.BertTokenizer, './bert-base-chinese/')         \n",
    "        self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "        self.bert = model_class.from_pretrained(pretrained_weights)\n",
    "        self.dense = nn.Linear(768, 2)  #bert默认的隐藏单元数是768， 输出单元是2，表示二分类\n",
    "        \n",
    "    def forward(self, batch_sentences):\n",
    "        #print(batch_sentences[4])\n",
    "        batch_tokenized = self.tokenizer.batch_encode_plus(batch_sentences, add_special_tokens=True, pad_to_max_length=True, truncation=True, max_length=512)      #tokenize、add special token、pad\n",
    "        # batch_tokenized = self.tokenizer.batch_encode_plus(batch_sentences, add_special_tokens=True,max_len=66, pad_to_max_length=True)      #tokenize、add special token、pad\n",
    "        input_ids = torch.tensor(batch_tokenized['input_ids'])\n",
    "        attention_mask = torch.tensor(batch_tokenized['attention_mask'])\n",
    "        bert_output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        bert_cls_hidden_state = bert_output[0][:,0,:]       #提取[CLS]对应的隐藏状态\n",
    "        linear_output = self.dense(bert_cls_hidden_state)\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "15000\n",
      "5000\n",
      "['手机一直用华为的，不错，分辨率没有苹果的好（要求有点高?）买来给孩子用的，非常好'\n",
      " '了解茨威格是是通过他的名作《一个陌生女人的来信》及《象棋的故事》，从来没有料想过哪位伟大的作家能把文学作品构局的如此紧凑严密，高潮迭起，洗练深刻，尤其是对人物的内心的深度剖析几近胜过当事人对自己情感的理解把握。对于这位虽然没有得过诺贝尔文学奖，但将德语文学提升到英语、法语一样地位的伟大文学家，敬仰让我禁不住好奇，于是在当当网上购买了他的自传——《茨威格自传》。茨威格的自传经得起对传记严格挑剔的朋友的巡检。他向我们展示了一位具有世界主义视野的思想家的历程。他青年时代起即对生命的价值与历史责任有明了的认识；他在早年选择了翻译这样默默无闻、费力不讨好的工作了解和掌握母语精髓；为了创作真正有价值的作品，他在多看、多学、多了解世界的本质后才开始真正的创作；他游历欧洲，拜访结交比利时诗人维尔哈、艺术家罗丹、爱尔兰诗人叶芝等许多伟大的人物，感触了那些伟大心灵，结下纯净真挚的友谊。他的内心世界始终是自由的，如同他最后自由的选择了死亡。我们不会以世俗的眼光判定他的自杀为懦弱为逃避，我个人认为他至少不是在只剩下生命的躯壳后被命运遗弃，而是对世界还有着强烈、鲜活的认识时尊严的作出了抉择。茨威格以为作家真正使命，是维护和保卫“一切人性”。本书中，作者跨越时光的藩篱重现的群星闪耀时刻，以不可思议的描述技巧讲述了七个不可思议的历史转折，将人性光辉的闪耀时刻予以展现，让他们如星辰般永恒的人性光辉，给我们凡俗的生活以正义与自由的启明。'\n",
      " '宝贝已收到了！！！！感觉还可以！！！就是不知道能用多久！！！'\n",
      " '声音太小屏幕易磨 我的工作环境太吵,基本上就接不到电话,开着震动也不行,常耽误工作,我这机器是别人送的,让我自己买,我还是选诺基亚'\n",
      " '画面不清晰，跟手机没区别' '好好好好好好' '不好，说送东西什么多没有'\n",
      " '本来很喜欢这部卡通的，说明上写的清清楚楚是中英双语对白，中英双语字幕，绝无重复。买回来一看，里面竞然还有米老鼠唐老鸭不说，根本就没有中文对白和字幕，更气人的是，还有重复的。太缺德了。骗子！'\n",
      " '买的时候说做活动1 899，还没收到货就变成原价了1 799，然后申请保价，以有赠品为拒绝，再次进商品界面20 99，然后又是1 899。你们京东当消费者是**吗？东西马马虎虎吧，像素一般！要不是给家人买的，家人说算了绝对退货'\n",
      " '本身技术不好，4月上市我是11月买的。在保修期就坏了，老是死机。去维修部叫我等15天。结果还是没有修好，拿回来声音有问题，拆东墙补西墙。敷衍我。于是我又去，就这样拖来拖去。过了保修期了。售后技术，效率，服务差。手机本身质量有问题。浪费我1590块，我恨死他了。我的朋友也不敢买这个牌子了。谁来付我的损失。你们还敢买吗？'\n",
      " '垃圾，内存小的很，主要的是内存越用越小，装俩三个软件就没内存了，清理都清理不出来。根本没法用，实在太垃圾了'\n",
      " '也买了，感觉不错，手感很好，2280搞定,送蓝牙' '外形好看' '垃圾中的极品'\n",
      " '个头不大，甜酸适度，过节仍送到家，京东暖心服务！给一百个赞！！！'\n",
      " '看不懂，不管买什么都平板，都送耳机的吧，买个平板原本很高兴！打开看就一个充电头数据线，没耳机！还说什么标配，怎么不把这些配置写在购物里面！**真的希望'\n",
      " '给上中学的孩子买的。收到之后，自己先浏览了一遍，感觉书写得不错，可以帮助成长中的、好奇心极强的中学生们了解自己生理和心理的特点，以及如何交友和远离有害的东西。书写得很好，也很全面。我是农村长大的，小时候，根本没有看过这一类的书，所以也走了很多弯路。值得成长中的中学生一看。家长也可以看一下，可以指导处于青春期的孩子们成长。'\n",
      " '第一次买屏幕里有个灰尘，退了重新买了个，第二次竟然还有个，比较庆幸的是比上次那个小，懒得再退了，不过好评没戏了' '物流服务非常满意！！'\n",
      " '不错，妈妈很喜欢'\n",
      " '指纹解锁很快，系统很流畅，音质非常好，4G运行内存剩下2.5G，平板内存系统占的有点多哈，电池很耐用，第一次用京东买东西，在我这里京东快递比顺丰还快。遇到搞活动买的，什么也没送（有一张膜），打4分吧，留一分代表有进步的空间'\n",
      " '国产品牌、物美价廉，发货及时、迅速快捷。包装规范、完好无损，客服耐心、认真负责。'\n",
      " '其实除了没闪光灯外是好机子，那为什么要给评分低呢？？？？？京东你说什么春节不打烊，做不到就不要说！二十九拍时说次日到，结果直接初五才到！！！！！'\n",
      " '手机还行:上网挺快的:就是没给开发票!'\n",
      " '苹果很好吃很香，又甜又脆的，赶上搞活动买的，一直都很喜欢吃，已经多次购买了，喜欢的亲们赶紧下单吧，物流快，包装的很好，没什么坏果，赞赞赞'\n",
      " '还补充一点就是,家长要和小朋友一起看,引导小朋友想出更多奇妙的东西,这才是主要的,书中教了的方法,家长要引申出来,希望小朋友可以尽情发挥他们丰富的想象.让他们想得更多,动脑动手得更多'\n",
      " '刚到手打开一看机身有一个划痕，看电影点不出来快进横条，全屏的时候一边黑一边白只能选择旋转才没有\\ue411 。'\n",
      " '总体来说不错，期间降价给我退了差价，给京东点个赞！' '我女儿很喜欢.画面漂亮、有趣，内容也很精彩。连我都很喜欢。值得一读。'\n",
      " '刚买就降* ，要不要这么坑'\n",
      " '今年五一买的，待机方面虚假宣传,手机盒上有\"超长待机,不用天天充电\"的标志,说明书明示570mah电池的待机时间为100-200小时,过了两天后我又到买机器地去逛,海尔柜台上仍然悬挂着\"待机8-12天\"的海报.而实际使用情况是,我平时的电话并不多,每天夜间关机,待机时间短的时候不到两天,长的时候也不到四天,实际待机时间只有30-60小时,差距有些太大了吧？手机完全不使用的情况没有试过,但100-200小时的待机时间应该是不可能地。'\n",
      " '总体感觉不错,这个价格的功能还说得过去.' '挺好。。。。。。。'\n",
      " '可以看出作者很用心，绘图细致精美，人物形象可爱，我很喜欢这只小狐狸~看《妈妈》这一章的时候，父母刚好都出差了，家里就我一人，所以看着看着就哭了（我已经一年多没哭过了）总体上非常好，不过因为我拿到的是崭新的书，所以刚打开时，油墨的味道很刺鼻，让人很难受。而且我看完整本书只花了10分钟，毕竟只是漫画书嘛，想追求丰富文字内容的朋友可能会失望……建议比较有“闲钱”的朋友购买~~'\n",
      " '在用816这部机子，开机和关机都有音乐在响，开始还感觉不错，但是后来就有点烦了，想关掉，同寝室的室友也有意见了，总说吵呢，送到客服去，小姐说这个关不掉，哭死，现在早早就关机了，怕吵到别人，不过这样倒是省电了，一般我能用到3到4天。如果打电话会很累人，对方总说听不清我说话，现在想换别的机子了'\n",
      " '很轻 开机速度很快 看视频十分方便 漂亮' '京东就是不一样，还先给你试下耳机孔是不是能用。服务态度却是可以'\n",
      " '也许我水平太低,我不喜欢此书.整书都在吹嘘.' '苹果不大哦，目测没有80，出于对自营信赖有限好评'\n",
      " '电池不行，反应问题商家也不管，还说是正常' '送货超快，价格实惠，没有坏的，味道不错 ，就是个头小了点儿。'\n",
      " '屏保很花。不知道是什么原因。感觉跟荣耀x1差很远。连普通发票都没有。怀疑不是正品。'\n",
      " '还没吃，包装很好，没有烂的，摸上去冰冰的，觉得80mm有点小……没想象这么大，应该挺好吃的吧，***一斤也差不多就是市场价了。'\n",
      " '送货快，新鲜，真的很新鲜，还带有水珠，味道香甜，水分足，好吃，值得推荐。包装很好，没有磕碰。快递小哥态度特别好'\n",
      " '东西不错，屏幕清晰，速度快，大小合适，看电视，玩游戏都不错，快递师傅很热情，下单送货速度很快' '普通的苹果，比在外面买便宜，也新鲜'\n",
      " '包装较差 这么一个平板里面连个防震气垫都不舍得放 就光秃秃的一个商品放在盒子里'\n",
      " '用了几天才来评价，以下是切身体会1.画质和音效不错，看视频啥的体验都很棒，并且比较轻薄2.耗电量：我主要用来看视频和看文档之类的，主要用来学习，差不多一天多（晚上还是也让它休息一下?），如果降低了分辨率，开启省电模式，会用的更久一点3.带上耳机会听到电流声，外放的话基本可以忽略4.最不满意的是指纹解锁，个人觉得不太灵敏，有很多次都是验证失败了，很影响心情，但用我的iphone就不会，秒秒钟开。5.屏幕比例个人觉得不太适合看文档，当然这是个人感觉，可能是我用电脑习惯了吧，但是有护眼模式值得表扬??7.可能是软件不兼容吧，我在微信上视频会有很大很大的回声，完全受不了，但是在qq上视频还好8.网上说的啥子拖影我感觉不出来，可能是我太木了吧?9.是请要下单的朋友注意，送的贴膜如果自己手残就让会贴膜的贴，这个屏幕太大了，跟手机贴膜的难度是不一样的，我就贴毁了，有5、6个气泡，但是也不想重新换了?10.鲁大师跑分12万多，说是正品?'\n",
      " '东西整体性能不错，特别是音乐和视频，非常棒，办公软件也比较好用，文档和表格能满足基本需要，本人不玩游戏，网上说的拖影确实存在，希望以后的产品有所改进'\n",
      " '180度旋转摄像头，手写触摸屏幕，支持MP3铃声，支持多媒体影音播放'\n",
      " '不知道买了多少次的水果，除了涨了几块钱，质量依旧有保障。感谢快递小哥送货上门。' '当当真差劲，我订购5本高木直子的书，发给我5本穿越文！'\n",
      " '内存占用太多，剩下了几个G的内存，算是比较满意吧。'\n",
      " '银色的机身，看上去时尚！体积还算小，方便携带！做工感觉很好，很细致，无可挑剔！放在手里的感觉相当好！耳机与充电接口都有防尘盖！文本的颜色可自己调，这是有极大好处的，这点也不错，有时壁纸的颜色很可能和文本颜色有重合，现在换一下，来电号码看得就很清楚了。'\n",
      " '坑 *人 ~骗* 子的垃 圾手机，说是可用移动联通加SD卡，买回来却只取其一，说什么7天退货，网页点击不开的收货地址！等过了7天之后就能点开网页了！分明~坑~爹！'\n",
      " '质量很好，价格合适，性价比超高，很好用，下次还来京东购买。'\n",
      " '屏幕效果很好！很清晰，这点最令人满意！接口有橡胶，可以避免尘土进去，减少机子的寿命！支持多种铃声，一般格式的音乐都可以播放！发彩信也放便，不仅能发图片，还能发音乐和视频；而且速度也挺快的，没有因为文件大就慢腾腾的；能够即时和朋友分享到乐趣！还有，用着感觉功能比较齐全，没有缺少什么或借助其它的需求！'\n",
      " '性价比高，发货速度快，试用二天不错！'\n",
      " '手机产品都很满意，唯一不满意的是堂堂华为为何学习小米饥饿营销？差评给华为和京东饥饿营销的，每次都没货，我勒个去，等了几个月金色版！一款好的产品，不要去在乎它的营销手段，要好好学些人家苹果的销售模式，从来不搞饥饿营销！鄙人认为华为很快会成为国产手机第一，甩小米几条街，小米的做工恐怕和华为没法比，包括核心硬件小米都没法和华为抗衡！这款X2平板手机很不错，电池对于我这个游戏迷还是不够用一天冲一次，当然你就偶尔看看网页什么的管2天应该没问题，屏幕和苹果的不差上下，显示很犀利，说看视频屏幕不清晰的是傻B，你去下载个1080P的电影看看就明白了，保证清晰！金属材质很有质感，有点大，不方便带出门，不喜欢拎包包的恐怕要拿在手上了，外出不方便，等了很多天才在6月18抢到手2399，天天显示无货，华为官网上却天天在卖，你个京东太没魄力了，这点货都拿不到，真的鄙视，还战略合作，都在相互坑骗！最后重申产品没问题，差评给京东和华为饥饿营销的！'\n",
      " '很好！送货快！态度好！机子好看好用！' '好吃啊，好吃啊，好吃啊，这是第三次，吃完又来买了。'\n",
      " '很好很棒很满意，京东就是靠谱，包装讲究，水果很新鲜很爽脆！    很好很棒很满意，京东就是靠谱，包装讲究，水果很新鲜很爽脆！'\n",
      " '你们自己看图？我真的不懂，为什么明明没有USB接口非得写成这样，害得我才买了一个USB键盘，然后客服就是不给说法和处理方法。再说到磕碰问题，就开始推责任踢皮球！我要的是处理解决方案！拜托！拜托？华为！京东！！！'\n",
      " '很不借，苹果色泽很好。包装非常好。大家看图片就知道了。京东就是好，快递员很礼貌。']\n"
     ]
    }
   ],
   "source": [
    "sentences = train_set['comments'].values\n",
    "targets = train_set['label'].values\n",
    "print(len(train_set))\n",
    "train_inputs, test_inputs, train_targets, test_targets = train_test_split(sentences, targets, random_state = 42)\n",
    "print(len(train_inputs))\n",
    "print(len(test_inputs))\n",
    "batch_size = 64\n",
    "batch_count = int(len(train_inputs) / batch_size)\n",
    "batch_train_inputs, batch_train_targets = [], []\n",
    "for i in range(batch_count):\n",
    "    batch_train_inputs.append(train_inputs[i*batch_size : (i+1)*batch_size])\n",
    "    batch_train_targets.append(train_targets[i*batch_size : (i+1)*batch_size])\n",
    "print(batch_train_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "Batch: 10, Loss: 0.8606\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "Batch: 20, Loss: 0.7063\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "#using GPU\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "epochs = 4\n",
    "lr = 0.01\n",
    "print_every_batch = 10\n",
    "bert_classifier_model = BertClassificationModel()\n",
    "optimizer = optim.SGD(bert_classifier_model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# model = bert_classifier_model\n",
    "# model.to(device)  #使用序号为0的GPU\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print_avg_loss = 0\n",
    "    for i in range(batch_count):\n",
    "        inputs = batch_train_inputs[i]\n",
    "        labels = torch.tensor(batch_train_targets[i])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = bert_classifier_model(inputs)\n",
    "        print(outputs.shape)\n",
    "        print(labels.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print_avg_loss += loss.item()\n",
    "        if i % print_every_batch == (print_every_batch-1):\n",
    "            print(\"Batch: %d, Loss: %.4f\" % ((i+1), print_avg_loss/print_every_batch))\n",
    "            print_avg_loss = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "-----------------------------------------\n",
      "0 comment: 味道很好……就是饮料机那天坏了\n",
      "0 predictive value: 0\n",
      "wrong prediction\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (555) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-c13b326efccd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_classifier_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtest_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HDD/anaconda3/envs/bert/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-81a716b4062d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_sentences)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mbert_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mbert_cls_hidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m       \u001b[0;31m#提取[CLS]对应的隐藏状态\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mlinear_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_cls_hidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HDD/anaconda3/envs/bert/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HDD/anaconda3/envs/bert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    960\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HDD/anaconda3/envs/bert/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HDD/anaconda3/envs/bert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"absolute\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (555) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# eval the trained model\n",
    "total = len(test_inputs)\n",
    "print(len(test_inputs))\n",
    "hit = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(total):\n",
    "        outputs = bert_classifier_model([test_inputs[i]])\n",
    "        _,predicted = torch.max(outputs, 1)\n",
    "        if predicted == test_targets[i]:\n",
    "            hit += 1\n",
    "        if (i % 100 == 0) :\n",
    "            print(\"-----------------------------------------\")\n",
    "            print(\"%d comment: %s\" % (i, test_inputs[i]))\n",
    "            print(\"%d predictive value: %d\" % (i, predicted))\n",
    "            print(\"right prediction\" if (predicted == test_targets[i]) else \"wrong prediction\")\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (hit / total * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}